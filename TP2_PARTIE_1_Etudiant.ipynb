{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2. PARTIE 1. scikit-learn + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module de Machine Learning en Python : scitkit-learn (sklearn)\n",
    "http://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan :\n",
    "\n",
    "   [- Iris dataset](#1)\n",
    "   \n",
    "   [- Naive Bayes](#2)\n",
    "   \n",
    "   [- Mon Naive Bayes](#3)\n",
    "   \n",
    "   [- Tests](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher le dataset **iris** dans le module sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Analyser les résultats des commandes suivantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': '/Users/agatheguilloux/anaconda3/lib/python3.6/site-packages/sklearn/datasets/data/iris.csv'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GaussianNB in module sklearn.naive_bayes:\n",
      "\n",
      "class GaussianNB(BaseNB)\n",
      " |  Gaussian Naive Bayes (GaussianNB)\n",
      " |  \n",
      " |  Can perform online updates to model parameters via `partial_fit` method.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like, shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  var_smoothing : float, optional (default=1e-9)\n",
      " |      Portion of the largest variance of all features that is added to\n",
      " |      variances for calculation stability.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_prior_ : array, shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  class_count_ : array, shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  theta_ : array, shape (n_classes, n_features)\n",
      " |      mean of each feature per class\n",
      " |  \n",
      " |  sigma_ : array, shape (n_classes, n_features)\n",
      " |      variance of each feature per class\n",
      " |  \n",
      " |  epsilon_ : float\n",
      " |      absolute additive value to variances\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB(priors=None, var_smoothing=1e-09)\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB(priors=None, var_smoothing=1e-09)\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      BaseNB\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, priors=None, var_smoothing=1e-09)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like, shape (n_classes,), optional (default=None)\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = [n_samples]\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module sklearn.naive_bayes:\n",
      "\n",
      "fit(X, y, sample_weight=None) method of sklearn.naive_bayes.GaussianNB instance\n",
      "    Fit Gaussian Naive Bayes according to X, y\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        Training vectors, where n_samples is the number of samples\n",
      "        and n_features is the number of features.\n",
      "    \n",
      "    y : array-like, shape (n_samples,)\n",
      "        Target values.\n",
      "    \n",
      "    sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      "        Weights applied to individual samples (1. for unweighted).\n",
      "    \n",
      "        .. versionadded:: 0.17\n",
      "           Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    self : object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "help(gnb.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On donne ici un exemple d'utilisation du Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (iris.data.shape[0],(iris.target != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) On sépare en 2 parties égales le dataset avec tirage aléatoire sans remise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109, 121,  96,  14, 108, 136,  82,  37, 146,  54,  99,  44, 133,\n",
       "        46,   0, 110, 141,  25,  88, 124, 138, 131,  22,  31,  69,   1,\n",
       "        20,  77,  33,  42,  59,  86, 115,  97,  65,  13, 112,  80,  28,\n",
       "        68,  94, 142,  67,  29,  98,  73, 125,  39,  79, 148,  63,  36,\n",
       "        11, 113,  16,  17,  76,  90, 132,  85,  32,  60,  51,  91,  27,\n",
       "        84,  40, 118,  47, 130,   9,  41,   2,   6, 144])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.random.choice(range(150), 75, replace=False)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   4,   5,   7,   8,  10,  12,  15,  18,  19,  21,  23,  24,\n",
       "        26,  30,  34,  35,  38,  43,  45,  48,  49,  50,  52,  53,  55,\n",
       "        56,  57,  58,  61,  62,  64,  66,  70,  71,  72,  74,  75,  78,\n",
       "        81,  83,  87,  89,  92,  93,  95, 100, 101, 102, 103, 104, 105,\n",
       "       106, 107, 111, 114, 116, 117, 119, 120, 122, 123, 126, 127, 128,\n",
       "       129, 134, 135, 137, 139, 140, 143, 145, 147, 149])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.delete(range(150), train)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on regarde le résutat sur la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.333333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gnb.fit(iris.data[train,], iris.target[train,]).predict(iris.data[test,])\n",
    "100*(iris.target[test,] != y_pred).sum()/len(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Construire une fonction NB de paramètre A, B et nb qui répète nb tirages aléatoires avec i données pour la partie apprentissage et 150-i pour le test avec i allant de A à B. La fonction renvoie une liste de taille B-A+1 avec le pourcentage de prédiction exacte sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 86,  99,  52,  65, 143])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(150), 5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48.648648648648646,\n",
       " 47.61904761904763,\n",
       " 54.337899543378995,\n",
       " 46.20689655172415,\n",
       " 51.15740740740741,\n",
       " 45.68764568764569,\n",
       " 41.07981220657277,\n",
       " 33.333333333333336,\n",
       " 22.14285714285714,\n",
       " 23.02158273381295,\n",
       " 6.763285024154588,\n",
       " 35.523114355231144,\n",
       " 9.068627450980392,\n",
       " 27.160493827160494,\n",
       " 8.457711442786069,\n",
       " 17.79448621553885,\n",
       " 5.808080808080809,\n",
       " 4.8346055979643765,\n",
       " 8.461538461538462,\n",
       " 13.953488372093025,\n",
       " 9.635416666666666,\n",
       " 9.186351706036746,\n",
       " 6.613756613756614,\n",
       " 6.933333333333333,\n",
       " 5.645161290322581,\n",
       " 5.420054200542005,\n",
       " 7.923497267759562,\n",
       " 6.06060606060606,\n",
       " 7.500000000000001]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NB(A,B,nb):\n",
    "    res = []\n",
    "    for i in range(A,B+1):\n",
    "        temp = 0\n",
    "        for j in range(nb):\n",
    "            train = np.random.choice(range(150), i, replace=False)\n",
    "            test = np.delete(range(150), train)\n",
    "            y_pred = gnb.fit(iris.data[train,], iris.target[train,]).predict(iris.data[test,])\n",
    "            temp = temp + 100*(iris.target[test,] != y_pred).sum()/len(test)\n",
    "        res = res + [temp/nb]\n",
    "    return res\n",
    "NB(2,30,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Tracer sur un graphique le vecteur NB(A,B,10) avec A = 2 et B = 149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = 2\n",
    "B = 149\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.linspace(A,B,B-A+1), NB(A,B,100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mon Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Si Bayes est si naif que ça, pourquoi pas le construire nous-même ! En s'aidant du cours, construire la fonction MonNaiveBayes sur le modèle suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MonNaiveBayes(train, test):\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Comparer le résultat au résultat de la fonction de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1e7d61b61710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0miris2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0miris2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mMonNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a4c72c0354c1>\u001b[0m in \u001b[0;36mMonNaiveBayes\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mMonNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "iris2 = copy.deepcopy(iris)\n",
    "\n",
    "iris2.data = iris2.data[3:30,]\n",
    "iris2.target = iris2.target[3:30,]\n",
    "MonNaiveBayes(iris, iris2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
